{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "domaci4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOzwN8DdAnRAp1Hv9/TrAO+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikolaZubic/AppliedGameTheoryHomeworkSolutions/blob/main/domaci4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1gBcEtr4n9I"
      },
      "source": [
        "# ČETVRTI DOMAĆI ZADATAK iz predmeta \"Primenjena teorija igara\" (Applied Game Theory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgcbjgtT4rUI"
      },
      "source": [
        "a) Poređenje rezultata kod pristupa Q-learning, SARSA i Monte Karlo metoda za igru Ajnc (BlackJack): https://github.com/NikolaZubic/AppliedGameTheoryHomeworkSolutions/blob/main/Homework%203/domaci3.ipynb\n",
        "\n",
        "b) Razvoj bota za igranje igre Iks-Oks (Tic Tac Toe) koristeći \"Q-learning\" pristup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFRnmA9y5r8c"
      },
      "source": [
        "# a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqj8eq__5tTT"
      },
      "source": [
        "Suštinska razlika između navedenih metoda jeste u načinu ažuriranja Q-vrijednosti. Uzeto je da Q-learning i SARSA koriste Temporal-Difference Learning, sa time da kod Q-learninga u narednom stanju uvijek biramo najbolju akciju, dok kod SARSA algoritma biramo akciju na osnovu politike odlučivanja.<br>\n",
        "\n",
        "Dobijeni su sledeći rezultati:\n",
        "\n",
        "Q LEARNING:<br>\n",
        "Player wins: 3681<br>\n",
        "Dealer wins: 5706<br>\n",
        "Player wins percentage = 39.21%<br>\n",
        "<br>\n",
        "SARSA:<br>\n",
        "Player wins: 3647<br>\n",
        "Dealer wins: 5740<br>\n",
        "Player wins percentage = 38.85%<br>\n",
        "<br>\n",
        "MONTE-CARLO METHODS:<br>\n",
        "Player wins: 3314<br>\n",
        "Dealer wins: 6186<br>\n",
        "Player wins percentage = 34.88%<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXxs-U9l63aQ"
      },
      "source": [
        "# b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0WJIoIG65n9"
      },
      "source": [
        "# Potrebni import-i"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaV9gYj-68Wk"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "import pickle\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvFYLOsb7SSu"
      },
      "source": [
        "# Definisanje Iks-Oks okruženja koristeći \"Open AI Gym\" toolkit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bRLoWc67TzO"
      },
      "source": [
        "class TicTacToeEnvironment(gym.Env):\n",
        "    # Because of human-friendly output\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, player_1, player_2):\n",
        "        \"\"\"\n",
        "        Board is predefined to a 3 x 3 grid.\n",
        "        We keep track of whether the game is over.\n",
        "        When initializing the TicTacToeEnvironment, we set game_over flag to 'False'.\n",
        "        :param player_1: First player\n",
        "        :param player_2: Second player\n",
        "        \"\"\"\n",
        "        self.observation_space = spaces.Discrete(3 * 3)\n",
        "        self.action_space = spaces.Discrete(9)\n",
        "        self.board = np.zeros((3, 3))\n",
        "        self.player_1 = player_1\n",
        "        self.player_2 = player_2\n",
        "        self.game_over = False\n",
        "        self.reset()\n",
        "\n",
        "        # Let player_1 play first\n",
        "        self.current_player = 1\n",
        "\n",
        "        # Board string representation\n",
        "        self.board_str = None\n",
        "\n",
        "    def reset(self):\n",
        "        # Resets the environment after one game.\n",
        "        self.board = np.zeros((3, 3))  # set board to zeros\n",
        "        self.board_str = None  # set board string representation to null\n",
        "        self.game_over = False\n",
        "        self.current_player = 1\n",
        "\n",
        "    def get_board(self):\n",
        "        # getter for current board\n",
        "        return self.board\n",
        "\n",
        "    def get_board_str(self):\n",
        "        # synchronize board string representation with current board state\n",
        "        self.board_str = str(self.board.reshape(3 * 3))\n",
        "        return self.board_str\n",
        "\n",
        "    def get_free_positions(self):\n",
        "        # return positions on the board that are free / not occupied\n",
        "        positions = [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == 0]\n",
        "        return positions\n",
        "\n",
        "    def update_state(self, new_position):\n",
        "        \"\"\"\n",
        "        Update the current state of the board. First player puts '1' on the board, second player puts '-1' on the board.\n",
        "\n",
        "        :param new_position: from set { (0, 0), (0, 1), (0, 2), (1, 0,), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2) }\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        self.board[new_position] = self.current_player\n",
        "\n",
        "        # If current player puts 1 (\"X\") on the board, next move is player 2 who puts -1 \"O\" on the board.\n",
        "        if self.current_player == 1:\n",
        "            self.current_player = -1\n",
        "        else:\n",
        "            self.current_player = 1\n",
        "\n",
        "    def check_game_status(self):\n",
        "        \"\"\"\n",
        "        Check if game has finished.\n",
        "\n",
        "        :return: 1 if player 1 has won,\n",
        "                 -1 if player 2 has won,\n",
        "                 0 if draw,\n",
        "                 None if game hasn't finished.\n",
        "        \"\"\"\n",
        "        for i in range(3):\n",
        "            if sum(self.board[i, :]) == 3:\n",
        "                self.game_over = True\n",
        "                return 1\n",
        "            if sum(self.board[i, :]) == -3:\n",
        "                self.game_over = True\n",
        "                return - 1\n",
        "\n",
        "        for i in range(3):\n",
        "            if sum(self.board[:, i]) == 3:\n",
        "                self.game_over = True\n",
        "                return 1\n",
        "            if sum(self.board[:, i]) == -3:\n",
        "                self.game_over = True\n",
        "                return -1\n",
        "\n",
        "        main_diagonal_sum = sum([self.board[i, i] for i in range(3)])\n",
        "        anti_diagonal_sum = sum([self.board[i, 3 - i - 1] for i in range(3)])\n",
        "        diagonal_sum = max(abs(main_diagonal_sum), abs(anti_diagonal_sum))\n",
        "\n",
        "        if diagonal_sum == 3:\n",
        "            self.game_over = True\n",
        "\n",
        "            if diagonal_sum == 3 or anti_diagonal_sum == 3:\n",
        "                return 1\n",
        "            else:\n",
        "                return - 1\n",
        "\n",
        "        # DRAW\n",
        "        if len(self.get_free_positions()) == 0:\n",
        "            self.game_over = True\n",
        "            return 0\n",
        "\n",
        "        self.game_over = False\n",
        "\n",
        "        return None\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Performs one action.\n",
        "\n",
        "        :param action: from set { (0, 0), (0, 1), (0, 2), (1, 0,), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2) }\n",
        "        :return: current board state, reward, boolean indicating whether the game is over,\n",
        "                 information about won the game if over\n",
        "        \"\"\"\n",
        "        if self.game_over:\n",
        "            return self.board, 0, True, None\n",
        "\n",
        "        self.update_state(action)\n",
        "        current_game_status = self.check_game_status()\n",
        "\n",
        "        if current_game_status is not None:\n",
        "            if current_game_status == 1:\n",
        "                reward = 1\n",
        "                info = {\"Result\": \"Player 1 won the game.\"}\n",
        "            elif current_game_status == -1:\n",
        "                reward = -1\n",
        "                info = {\"Result\": \"Player 2 won the game.\"}\n",
        "            else:\n",
        "                # DRAW\n",
        "                reward = 0\n",
        "                info = {\"Result\": \"Draw.\"}\n",
        "            return self.board, reward, self.game_over, info\n",
        "        return self.board, None, self.game_over, None\n",
        "\n",
        "    def render(self):\n",
        "        for i in range(0, 3):\n",
        "            print('-------------')\n",
        "            out = '| '\n",
        "            for j in range(0, 3):\n",
        "                token = ''\n",
        "                if self.board[i, j] == 1:\n",
        "                    token = 'X'\n",
        "                if self.board[i, j] == -1:\n",
        "                    token = 'O'\n",
        "                if self.board[i, j] == 0:\n",
        "                    token = ' '\n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print('-------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SFvkeK37ahH"
      },
      "source": [
        "# Definisanje Q-learning agenta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t20fWNQI7dlo"
      },
      "source": [
        "class QLearningAgent(object):\n",
        "    def __init__(self, gamma=0.9, LAMBDA=0.1, epsilon=0.1):\n",
        "        self.gamma = gamma  # Discounting factor\n",
        "        self.LAMBDA = LAMBDA  # Used for filtering q-values, learning rate\n",
        "        self.epsilon = epsilon  # Used in epsilon-greedy policy\n",
        "        self.states = []\n",
        "        self.states_values = {}  # KEY = CURRENT_BOARD_STATE, VALUE = number\n",
        "\n",
        "    @staticmethod\n",
        "    def get_board_str(board):\n",
        "        return str(board.reshape(3 * 3))\n",
        "\n",
        "    def reset(self):\n",
        "        self.states = []\n",
        "\n",
        "    def add_state(self, state):\n",
        "        self.states.append(state)\n",
        "\n",
        "    def get_max_action(self, positions, current_board, symbol):\n",
        "        \"\"\"\n",
        "        Choose best action.\n",
        "\n",
        "        :param positions: set of free positions\n",
        "        :param current_board: current environment board\n",
        "        :param symbol: current player symbol (1 = \"X\" = PC and -1 = \"O\" = HUMAN)\n",
        "        :return: action, for example (1, 1) which is in the center of 3x3 grid\n",
        "        \"\"\"\n",
        "        if np.random.uniform(0, 1) <= self.epsilon:\n",
        "            idx = np.random.choice(len(positions))  # choose index from the interval [0, 9]\n",
        "            action = positions[idx]  # choose certain action, for example positions[1] = (0, 2)\n",
        "        else:\n",
        "            action = None\n",
        "            value_max = - np.inf\n",
        "\n",
        "            for pos in positions:\n",
        "                next_board = current_board.copy()\n",
        "                next_board[pos] = symbol\n",
        "                next_board_str = self.get_board_str(next_board)\n",
        "\n",
        "                if self.states_values.get(next_board_str) is None:\n",
        "                    action_value = 0\n",
        "                else:\n",
        "                    action_value = self.states_values.get(next_board_str)\n",
        "\n",
        "                if action_value >= value_max:\n",
        "                    value_max = action_value\n",
        "                    action = pos\n",
        "\n",
        "        return action\n",
        "\n",
        "    def compute_q(self, reward):\n",
        "        \"\"\"\n",
        "        Compute q values at the end of the game, all states are saved in self.states.\n",
        "\n",
        "        :param reward: depends on whether the player has won/lost/draw the game\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        for current_board_state in reversed(self.states):\n",
        "            if self.states_values.get(current_board_state) is None:\n",
        "                self.states_values[current_board_state] = 0\n",
        "\n",
        "            q_current_s_a = self.states_values[current_board_state]\n",
        "\n",
        "            # compute new q\n",
        "            self.states_values[current_board_state] = q_current_s_a + self.LAMBDA * (self.gamma * reward -\n",
        "                                                                                     q_current_s_a)\n",
        "\n",
        "            reward = self.states_values[current_board_state]\n",
        "\n",
        "    def save_policy(self):\n",
        "        file = open(\"saved_policy\", \"wb\")\n",
        "        pickle.dump(self.states_values, file)\n",
        "        file.close()\n",
        "\n",
        "    def load_policy(self, file_name):\n",
        "        file = open(file_name, \"rb\")\n",
        "        self.states_values = pickle.load(file)\n",
        "        file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05oPmzf97kxH"
      },
      "source": [
        "# Definisanje igrača, unos sa tastature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRu-7eBL7m4g"
      },
      "source": [
        "class HumanPlayer(object):\n",
        "    @staticmethod\n",
        "    def act(positions):\n",
        "        while True:\n",
        "            user_input = input(\"['O' on move] x,y: \")\n",
        "            x, y = user_input.split(\",\")\n",
        "            x_int, y_int = int(x) - 1, int(y) - 1\n",
        "            pos = (x_int, y_int)\n",
        "            if pos in positions:\n",
        "                return pos\n",
        "            else:\n",
        "                print(\"Invalid move. Try again.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41HjC8Gi7udo"
      },
      "source": [
        "# Treniranje agenta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeewXng67vsw"
      },
      "source": [
        "def train(number_of_episodes):\n",
        "    player_1 = QLearningAgent()\n",
        "    player_2 = QLearningAgent()\n",
        "    agents = [player_1, player_2]\n",
        "\n",
        "    environment = TicTacToeEnvironment(player_1, player_2)\n",
        "\n",
        "    for i in range(number_of_episodes):\n",
        "        if i % 100000 == 0:\n",
        "            print(\"Episode {}\".format(i))\n",
        "\n",
        "        environment.reset()\n",
        "\n",
        "        is_done = False\n",
        "\n",
        "        while not is_done:\n",
        "            for agent in agents:\n",
        "                if not is_done:\n",
        "                    free_positions = environment.get_free_positions()\n",
        "                    current_action = agent.get_max_action(free_positions, environment.get_board(),\n",
        "                                                          environment.current_player)\n",
        "                    current_board_state, reward, is_done, information = environment.step(current_action)\n",
        "                    agent.add_state(agent.get_board_str(current_board_state))\n",
        "\n",
        "                    if is_done:\n",
        "                        if reward == 1:\n",
        "                            # If player one wins, he gets the reward of 1\n",
        "                            player_1.compute_q(1)\n",
        "                            player_2.compute_q(0)\n",
        "                        elif reward == -1:\n",
        "                            # If player two wins, he gets the reward of 1\n",
        "                            player_1.compute_q(0)\n",
        "                            player_2.compute_q(1)\n",
        "                        else:\n",
        "                            # DRAW, computer gets smaller reward\n",
        "                            player_1.compute_q(0.1)\n",
        "                            player_2.compute_q(0.5)\n",
        "\n",
        "                        # At the end, reset history of board states for both players\n",
        "                        player_1.reset()\n",
        "                        player_2.reset()\n",
        "\n",
        "        environment.reset()\n",
        "    player_1.save_policy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c4LNUc572bJ"
      },
      "source": [
        "# Pokretanje igre"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Opv4SQjq73ug"
      },
      "source": [
        "def play_game():\n",
        "    player_1 = QLearningAgent()\n",
        "    player_1.load_policy(\"saved_policy\")\n",
        "    player_2 = HumanPlayer()\n",
        "    agents = [player_1, player_2]\n",
        "\n",
        "    environment = TicTacToeEnvironment(player_1, player_2)\n",
        "    environment.reset()\n",
        "\n",
        "    is_done = False\n",
        "    environment.render()\n",
        "\n",
        "    while not is_done:\n",
        "        for agent in agents:\n",
        "            if agent == player_1:\n",
        "                action = agent.get_max_action(environment.get_free_positions(), environment.get_board(),\n",
        "                                              environment.current_player)\n",
        "            else:\n",
        "                action = agent.act(environment.get_free_positions())\n",
        "            state, reward, is_done, information = environment.step(action)\n",
        "            environment.render()\n",
        "\n",
        "            if is_done:\n",
        "                print(information['Result'])\n",
        "                break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DxTqote75XQ"
      },
      "source": [
        "# Glavni program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEY7uzRJvI8c"
      },
      "source": [
        "Preuzimanje istreniranog modela (treniran kroz **1 milion epizoda/partija**):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr7qWPYQvNN-",
        "outputId": "680e73c9-b46e-4ebc-9871-aa337876d97a"
      },
      "source": [
        "!wget \"https://github.com/NikolaZubic/AppliedGameTheoryHomeworkSolutions/raw/main/Homework%204/saved_policy\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-13 20:18:41--  https://github.com/NikolaZubic/AppliedGameTheoryHomeworkSolutions/raw/main/Homework%204/saved_policy\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/NikolaZubic/AppliedGameTheoryHomeworkSolutions/main/Homework%204/saved_policy [following]\n",
            "--2020-12-13 20:18:41--  https://raw.githubusercontent.com/NikolaZubic/AppliedGameTheoryHomeworkSolutions/main/Homework%204/saved_policy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 146054 (143K) [application/octet-stream]\n",
            "Saving to: ‘saved_policy’\n",
            "\n",
            "\rsaved_policy          0%[                    ]       0  --.-KB/s               \rsaved_policy        100%[===================>] 142.63K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-12-13 20:18:41 (4.97 MB/s) - ‘saved_policy’ saved [146054/146054]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0ejLIXY79nx",
        "outputId": "edae7a2c-7e56-4ec1-d6a7-79911a5069f0"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    NUMBER_OF_EPISODES = 1000000\n",
        "\n",
        "    if not os.path.exists(\"saved_policy\"):\n",
        "        train(number_of_episodes=NUMBER_OF_EPISODES)\n",
        "    play_game()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   | X |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "['O' on move] x,y: 1,1\n",
            "-------------\n",
            "| O |   |   | \n",
            "-------------\n",
            "|   | X |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "-------------\n",
            "| O | X |   | \n",
            "-------------\n",
            "|   | X |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "['O' on move] x,y: 3,2\n",
            "-------------\n",
            "| O | X |   | \n",
            "-------------\n",
            "|   | X |   | \n",
            "-------------\n",
            "|   | O |   | \n",
            "-------------\n",
            "-------------\n",
            "| O | X |   | \n",
            "-------------\n",
            "|   | X |   | \n",
            "-------------\n",
            "| X | O |   | \n",
            "-------------\n",
            "['O' on move] x,y: 1,3\n",
            "-------------\n",
            "| O | X | O | \n",
            "-------------\n",
            "|   | X |   | \n",
            "-------------\n",
            "| X | O |   | \n",
            "-------------\n",
            "-------------\n",
            "| O | X | O | \n",
            "-------------\n",
            "| X | X |   | \n",
            "-------------\n",
            "| X | O |   | \n",
            "-------------\n",
            "['O' on move] x,y: 2,3\n",
            "-------------\n",
            "| O | X | O | \n",
            "-------------\n",
            "| X | X | O | \n",
            "-------------\n",
            "| X | O |   | \n",
            "-------------\n",
            "-------------\n",
            "| O | X | O | \n",
            "-------------\n",
            "| X | X | O | \n",
            "-------------\n",
            "| X | O | X | \n",
            "-------------\n",
            "Draw.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}